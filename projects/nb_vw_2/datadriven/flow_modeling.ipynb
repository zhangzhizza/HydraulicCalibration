{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10949419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "#from changedetect.kuncheva2014 import get_change_value\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7caf5dcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-2-60b214596ada>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-60b214596ada>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    reg1_x_21 = p1_2021_nonzero[['NB2_S_1_NYZ_cwp_9_HzSPR_x', 'NB2_S_1_NYZ_cwp_10_HzSPR_x',\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# regression\n",
    "    # 2020 data\n",
    "    # prepare training and validataion data set\n",
    "    \n",
    "    # 2021 data\n",
    "    # prepare training and validataion data set\n",
    "    reg1_x_21 = p1_2021_nonzero[['NB2_S_1_NYZ_cwp_9_HzSPR_x', 'NB2_S_1_NYZ_cwp_10_HzSPR_x',\n",
    "                          'NB2_S_1_NYZ_cwp_11_HzSPR_x', 'NB2_S_1_NYZ_cwp_12_HzSPR_x',\n",
    "                          'p_diff']].values\n",
    "    reg1_y_21 = p1_2021_nonzero['NB2_S_x_NYZ_x_x_Fcw_x'].values\n",
    "    reg1_data_len_21 = len(reg1_x_21)\n",
    "    reg1_train_len_21 = int(reg1_data_len_21 * 0.6)\n",
    "    reg1_x_train_21 = reg1_x_21[0: reg1_train_len_21]\n",
    "    reg1_x_valid_21 = reg1_x_21[reg1_train_len_21: ]\n",
    "    reg1_y_train_21 = reg1_y_21[0: reg1_train_len_21]\n",
    "    reg1_y_valid_21 = reg1_y_21[reg1_train_len_21: ]\n",
    "    regr.fit(reg1_x_20, reg1_y_20) # the only line for training\n",
    "    print('Training R: %s'%regr.score(reg1_x_train_20, reg1_y_train_20))\n",
    "    ##############################################################################\n",
    "    # all the following codes are for plotting\n",
    "    \n",
    "    # 2020 validation data plot\n",
    "    reg1_y_pred_20 = regr.predict(reg1_x_valid_20)\n",
    "    fig, ax=plt.subplots(figsize = (20, 10))\n",
    "    ax.plot(reg1_y_valid_20, color = 'blue', label = 'true')\n",
    "    ax.plot(reg1_y_pred_20, color = 'green', label = 'pred')\n",
    "    for x_i in range(0, len(reg1_y_pred_20), one_day_n):\n",
    "        ax.axvline(x=x_i, ymin=0, ymax=300)\n",
    "        ax.text(x=x_i, y=300, s='%s'%(x_i/one_day_n), fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.set_title('Validation using 2020 data')\n",
    "    fig.show()\n",
    "    print('Validation R: %s'%regr.score(reg1_x_valid_20, reg1_y_valid_20))\n",
    "    # 2020 validation data daily distribution change vs prediction error\n",
    "    day_range = 1 ################\n",
    "    daily_errors = []\n",
    "    daily_dist_changes = []\n",
    "    for day_i in range(0, int(len(reg1_y_valid_20)/one_day_n), day_range):\n",
    "        new_data = reg1_x_valid_20[day_i * one_day_n: (day_i + day_range) * one_day_n]\n",
    "        new_data_pred_y = regr.predict(new_data)\n",
    "        new_data_true_y = reg1_y_valid_20[day_i * one_day_n: (day_i + day_range) * one_day_n]\n",
    "        new_data_r2 = regr.score(new_data, new_data_true_y)#np.mean(np.abs(new_data_pred_y - new_data_true_y)/new_data_true_y)\n",
    "        daily_errors.append(new_data_r2)\n",
    "        change_val = get_change_value(org_data=reg1_x_train_20, new_data=new_data, \n",
    "                                     pca_exp_thres=0.25, cluster_n=3)\n",
    "        daily_dist_changes.append(change_val)\n",
    "    fig, ax=plt.subplots(figsize = (5, 5))\n",
    "    ax.scatter(daily_dist_changes, daily_errors)\n",
    "    ax.set_title('2020 validation data R2 vs. distribution change')\n",
    "    fig.show()\n",
    "    # print r2 and distribution change\n",
    "    r2dist_df = pd.DataFrame(np.array([daily_dist_changes, daily_errors]).T)\n",
    "    r2dist_df.columns = ['dist change', 'r2']\n",
    "    pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "    print(r2dist_df)\n",
    "    # 2021 test data plot\n",
    "    reg1_y_pred_21 = regr.predict(reg1_x_21)\n",
    "    fig, ax=plt.subplots(figsize = (20, 5))\n",
    "    ax.plot(reg1_y_21, color = 'blue', label = 'true')\n",
    "    ax.plot(reg1_y_pred_21, color = 'green', label = 'pred')\n",
    "    for x_i in range(0, len(reg1_y_21), one_day_n):\n",
    "        ax.axvline(x=x_i, ymin=0, ymax=300)\n",
    "        ax.text(x=x_i, y=300, s='%s'%(x_i/one_day_n), fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.set_title('Test using 2021 data')\n",
    "    fig.show()\n",
    "    print('Test in 2021 R: %s'%regr.score(reg1_x_21, reg1_y_21))\n",
    "    # distribution change\n",
    "    print('Distribution difference between 2020 training and validation dataset is: %s'%get_change_value(org_data=reg1_x_train_20, new_data=reg1_x_valid_20, \n",
    "                 pca_exp_thres=0.25, cluster_n=3))\n",
    "    print('Distribution difference between 2020 and 2021 dataset is: %s'%get_change_value(org_data=reg1_x_20, new_data=reg1_x_21, \n",
    "                 pca_exp_thres=0.25, cluster_n=3))\n",
    "    daily_dist_changes = []\n",
    "    daily_errors = []\n",
    "    print(reg1_data_len_21)\n",
    "    print(one_day_n)\n",
    "    for day_i in range(0, int(reg1_data_len_21/one_day_n), day_range):\n",
    "        new_data = reg1_x_21[day_i * one_day_n: (day_i + day_range) * one_day_n]\n",
    "        new_data_pred_y = regr.predict(new_data)\n",
    "        new_data_true_y = reg1_y_21[day_i * one_day_n: (day_i + day_range) * one_day_n]\n",
    "        new_data_r2 = regr.score(new_data, new_data_true_y)#np.mean(np.abs(new_data_pred_y - new_data_true_y)/new_data_true_y)\n",
    "        daily_errors.append(new_data_r2)\n",
    "        change_val = get_change_value(org_data=reg1_x_train_20, new_data=new_data, \n",
    "                                     pca_exp_thres=0.25, cluster_n=3)\n",
    "        daily_dist_changes.append(change_val)\n",
    "    fig, ax=plt.subplots(figsize = (5, 5))\n",
    "    ax.scatter(daily_dist_changes, daily_errors)\n",
    "    ax.set_title('R2 vs. distribution change')\n",
    "    fig.show()\n",
    "    # print r2 and distribution change\n",
    "    r2dist_df = pd.DataFrame(np.array([daily_dist_changes, daily_errors]).T)\n",
    "    r2dist_df.columns = ['dist change', 'r2']\n",
    "    pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "    print(r2dist_df)\n",
    "    # perform error analysis using multivariate distribution\n",
    "    reg1_x_20_mean = np.mean(reg1_x_20, axis = 0)\n",
    "    reg1_x_20_cov = np.cov(reg1_x_20, rowvar = False)\n",
    "    reg1_x_dist = sp.stats.multivariate_normal(mean = reg1_x_20_mean, \n",
    "                                         cov = reg1_x_20_cov)\n",
    "    valid20_dist2mean = []\n",
    "    for train_pt in reg1_x_valid_20:\n",
    "        test_dist2mean = np.matmul(np.matmul((train_pt - reg1_x_20_mean),\n",
    "                                     np.linalg.inv(reg1_x_20_cov)),\n",
    "                           (train_pt - reg1_x_20_mean).T)\n",
    "        valid20_dist2mean.append(test_dist2mean)\n",
    "    fig, ax=plt.subplots(figsize = (20, 5))\n",
    "    ax.plot(reg1_y_valid_20, color = 'blue', label = 'true')\n",
    "    ax.plot(reg1_y_pred_20, color = 'green', label = 'pred')\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(valid20_dist2mean, color = 'red', label = 'dist2mean')\n",
    "    ax.legend()\n",
    "    ax2.legend()\n",
    "    ax.set_title('Validation using 2020 data')\n",
    "    fig.show()\n",
    "    test21_dist2mean = []\n",
    "    test21_mapes = np.abs(reg1_y_pred_21 - reg1_y_21)/reg1_y_21\n",
    "    for train_pt in reg1_x_21:\n",
    "        test_dist2mean = np.matmul(np.matmul((train_pt - reg1_x_20_mean),\n",
    "                                     np.linalg.inv(reg1_x_20_cov)),\n",
    "                           (train_pt - reg1_x_20_mean).T)\n",
    "        test21_dist2mean.append(test_dist2mean)\n",
    "    fig, ax=plt.subplots(figsize = (20, 5))\n",
    "    ax.plot(reg1_y_21, color = 'blue', label = 'true')\n",
    "    ax.plot(reg1_y_pred_21, color = 'green', label = 'pred')\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(test21_dist2mean, color = 'red', label = 'dist2mean')\n",
    "    ax.legend()\n",
    "    ax.set_title('Test using 2021 data')\n",
    "    fig.show()\n",
    "    \n",
    "    x20_dist2mean = []\n",
    "    for train_pt in reg1_x_20:\n",
    "        test_dist2mean = np.matmul(np.matmul((train_pt - reg1_x_20_mean),\n",
    "                                     np.linalg.inv(reg1_x_20_cov)),\n",
    "                           (train_pt - reg1_x_20_mean).T)\n",
    "        x20_dist2mean.append(test_dist2mean)\n",
    "        \n",
    "    fig, ax=plt.subplots(figsize = (5, 5))\n",
    "    ax.scatter(test21_dist2mean, test21_mapes)\n",
    "    ax.set_title('dist2mean vs. mape')\n",
    "    fig.show()\n",
    "    # dist2mean cmp\n",
    "    fig, ax=plt.subplots(figsize = (20, 5))\n",
    "    ax.plot(x20_dist2mean, color = 'blue', label = '2020 dist2mean')\n",
    "    ax.plot(test21_dist2mean, color = 'green', label = '2021 dist2mean')\n",
    "    ax.legend()\n",
    "    ax.set_title('2020 vs 2021 dist2mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(groupby_n):\n",
    "    one_day_n = int(24*(60/groupby_n))\n",
    "    ######################## Year 2020 data #######################\n",
    "    # read flow data\n",
    "    p1_2020 = pd.read_csv('./raw_data/EndUserData2020P2.csv')\n",
    "    # perform time integration on data\n",
    "    p1_2020 = p1_2020.groupby(p1_2020.index // groupby_n).mean()\n",
    "    # select data with flow larger than 500 (ignore small flow data)\n",
    "    p1_2020_nonzero = p1_2020.loc[p1_2020['NB2_S_x_NYZ_x_x_Fcw_x']>500]\n",
    "    # read on/off state data\n",
    "    p1_2020_state = pd.read_csv('./raw_data/EndUserData2020P2CWPState.csv')\n",
    "    # perform time integration on data\n",
    "    p1_2020_state = p1_2020_state.groupby(p1_2020_state.index // groupby_n).mean()\n",
    "    # select data with flow larger than 500 (ignore small flow data)\n",
    "    p1_2020_state_nonzero = p1_2020_state.loc[p1_2020['NB2_S_x_NYZ_x_x_Fcw_x']>500]\n",
    "    # if the pump is off, set the frequency to zero\n",
    "    p1_2020_nonzero['NB2_S_1_NYZ_cwp_9_HzSPR_x'] = p1_2020_nonzero['NB2_S_1_NYZ_cwp_9_HzSPR_x']\\\n",
    "                                            * p1_2020_state_nonzero['NB2_S_1_NYZ_cwp_9_State_x']\n",
    "    p1_2020_nonzero['NB2_S_1_NYZ_cwp_10_HzSPR_x'] = p1_2020_nonzero['NB2_S_1_NYZ_cwp_10_HzSPR_x']\\\n",
    "                                            * p1_2020_state_nonzero['NB2_S_1_NYZ_cwp_10_State_x']\n",
    "    p1_2020_nonzero['NB2_S_1_NYZ_cwp_11_HzSPR_x'] = p1_2020_nonzero['NB2_S_1_NYZ_cwp_11_HzSPR_x']\\\n",
    "                                            * p1_2020_state_nonzero['NB2_S_1_NYZ_cwp_11_State_x']\n",
    "    p1_2020_nonzero['NB2_S_1_NYZ_cwp_12_HzSPR_x'] = p1_2020_nonzero['NB2_S_1_NYZ_cwp_12_HzSPR_x']\\\n",
    "                                            * p1_2020_state_nonzero['NB2_S_1_NYZ_cwp_12_State_x']\n",
    "    # get pressure difference\n",
    "    p1_2020_nonzero['p_diff'] =  p1_2020_nonzero['NB2_S_1_NYZ_sys_x_PcwOut_x'] - p1_2020_nonzero['NB2_S_1_NYZ_sys_x_PcwIn_x']\n",
    "    ######################## Year 2021 data ##########################\n",
    "    # same procedure apply to 2021 data\n",
    "    p1_2021 = pd.read_csv('./raw_data/EndUserData2021P2_fix.csv')\n",
    "    p1_2021_nonidx = np.where(pd.isnull(p1_2021).any(1) == 1)[0]\n",
    "    p1_2021 = p1_2021.drop(p1_2021_nonidx)\n",
    "    p1_2021 = p1_2021.groupby(p1_2021.index // groupby_n).mean()\n",
    "    p1_2021_nonzero = p1_2021.loc[p1_2021['NB2_S_x_NYZ_x_x_Fcw_x']>500]\n",
    "    \n",
    "    p1_2021_state = pd.read_csv('./raw_data/EndUserData2021P2CWPState.csv')\n",
    "    p1_2021_state = p1_2021_state.drop(p1_2021_nonidx)\n",
    "    p1_2021_state = p1_2021_state.groupby(p1_2021_state.index // groupby_n).mean()\n",
    "    p1_2021_state_nonzero = p1_2021_state.loc[p1_2021['NB2_S_x_NYZ_x_x_Fcw_x']>500]\n",
    "    \n",
    "    \n",
    "    p1_2021_nonzero['NB2_S_1_NYZ_cwp_9_HzSPR_x'] = p1_2021_nonzero['NB2_S_1_NYZ_cwp_9_HzSPR_x']\\\n",
    "                                            * p1_2021_state_nonzero['NB2_S_1_NYZ_cwp_9_State_x']\n",
    "    p1_2021_nonzero['NB2_S_1_NYZ_cwp_10_HzSPR_x'] = p1_2021_nonzero['NB2_S_1_NYZ_cwp_10_HzSPR_x']\\\n",
    "                                            * p1_2021_state_nonzero['NB2_S_1_NYZ_cwp_10_State_x']\n",
    "    p1_2021_nonzero['NB2_S_1_NYZ_cwp_11_HzSPR_x'] = p1_2021_nonzero['NB2_S_1_NYZ_cwp_11_HzSPR_x']\\\n",
    "                                            * p1_2021_state_nonzero['NB2_S_1_NYZ_cwp_11_State_x']\n",
    "    p1_2021_nonzero['NB2_S_1_NYZ_cwp_12_HzSPR_x'] = p1_2021_nonzero['NB2_S_1_NYZ_cwp_12_HzSPR_x']\\\n",
    "                                            * p1_2021_state_nonzero['NB2_S_1_NYZ_cwp_12_State_x']\n",
    "    p1_2021_nonzero['p_diff'] =  p1_2021_nonzero['NB2_S_1_NYZ_sys_x_PcwOut_x'] - p1_2021_nonzero['NB2_S_1_NYZ_sys_x_PcwIn_x']\n",
    "    ###################################################################################################################\n",
    "    return p1_2020_nonzero, p1_2021_nonzero\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def get_best_model(estimator: BaseEstimator,\n",
    "                   train_x: np.ndarray,\n",
    "                   train_y: np.ndarray,\n",
    "                   parameters_grid: dict,\n",
    "                   cv_n: int):\n",
    "    clf = GridSearchCV(estimator = estimator, \n",
    "                       param_grid = parameters_grid, \n",
    "                       cv = cv_n, \n",
    "                       scoring = 'neg_mean_absolute_error',\n",
    "                       verbose = 1)\n",
    "    clf.fit(train_x, train_y)\n",
    "    best_estimator = clf.best_estimator_\n",
    "    grid_res = clf.cv_results_\n",
    "    return best_estimator, grid_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54849b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "############### Read the raw data #############\n",
    "groupby_n = 60\n",
    "datadriven_res_dir = \"datadriven_res\"\n",
    "if not os.path.isdir(datadriven_res_dir):\n",
    "    os.mkdir(datadriven_res_dir)\n",
    "p1_20_df, p1_21_df = data_preprocess(groupby_n)\n",
    "\n",
    "############### Do grid serach for hyperparameter tuning ################\n",
    "# prepare different regressors\n",
    "estimators_dict = {}\n",
    "\n",
    "mlp_pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('mlp', MLPRegressor(hidden_layer_sizes=(100,), max_iter=2000, \n",
    "                            random_state=1, learning_rate = 'adaptive'))])\n",
    "mlp_parameters_grid = dict(mlp__hidden_layer_sizes=[(16, ), (32, ), (64, ), (128, ), (256,), \n",
    "                                                    (16, 16), (32, 32), (64, 64), (128, 128), (256, 256), \n",
    "                                                    (16, 16, 16), (32, 32, 32), (64, 64, 64), (128, 128, 128), (256, 256, 256)])\n",
    "estimators_dict['mlp'] = {'estimator': mlp_pipe, 'parameters_grid': mlp_parameters_grid}\n",
    "\n",
    "xgb_pipe = Pipeline([('scaler', StandardScaler()), ('xgb', xgb.XGBRegressor())])\n",
    "xgb_parameters_grid = dict(xgb__max_depth = [2, 4, 6, 8, 10, 12, 14, 16],\n",
    "                           xgb__n_estimators = [16, 32, 64, 128, 256, 512])\n",
    "estimators_dict['xgb'] = {'estimator': xgb_pipe, 'parameters_grid': xgb_parameters_grid}\n",
    "\n",
    "poly_pipe = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures()), ('ridge', Ridge())])\n",
    "poly_parameters_grid = dict(poly__degree = [2, 3, 4, 5],\n",
    "                           ridge__alpha = [0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "estimators_dict['poly'] = {'estimator': poly_pipe, 'parameters_grid': poly_parameters_grid}\n",
    "svr_pipe = Pipeline([('scaler', StandardScaler()), ('svr', SVR())])\n",
    "svr_parameters_grid = dict(svr__gamma = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100],\n",
    "                           svr__C = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100])\n",
    "estimators_dict['svr'] = {'estimator': svr_pipe, 'parameters_grid': svr_parameters_grid}\n",
    "# do grid search\n",
    "train_x = p1_20_df[['NB2_S_1_NYZ_cwp_9_HzSPR_x', 'NB2_S_1_NYZ_cwp_10_HzSPR_x',\n",
    "                          'NB2_S_1_NYZ_cwp_11_HzSPR_x', 'NB2_S_1_NYZ_cwp_12_HzSPR_x',\n",
    "                          'p_diff']].values\n",
    "train_y = p1_20_df['NB2_S_x_NYZ_x_x_Fcw_x'].values\n",
    "test_x = p1_21_df[['NB2_S_1_NYZ_cwp_9_HzSPR_x', 'NB2_S_1_NYZ_cwp_10_HzSPR_x',\n",
    "                          'NB2_S_1_NYZ_cwp_11_HzSPR_x', 'NB2_S_1_NYZ_cwp_12_HzSPR_x',\n",
    "                          'p_diff']].values\n",
    "test_y = p1_21_df['NB2_S_x_NYZ_x_x_Fcw_x'].values\n",
    "\n",
    "for estimator_key, estimator_val in estimators_dict.items():\n",
    "    estimator_i = estimator_val['estimator']\n",
    "    parameters_grid_i = estimator_val['parameters_grid']\n",
    "    best_model_i, grid_res_i = get_best_model(estimator = estimator_i,\n",
    "                       train_x = train_x,\n",
    "                       train_y = train_y,\n",
    "                       parameters_grid = parameters_grid_i,\n",
    "                       cv_n = 5)\n",
    "    train_ypred = best_model_i.predict(train_x)\n",
    "    test_ypred = best_model_i.predict(test_x)\n",
    "    train_res_df = pd.DataFrame(np.array([train_ypred, train_y]).T, columns = ['Fcw_ypred', 'Fcw_yobse'])\n",
    "    train_res_df['ts'] = p1_20_df['ts']\n",
    "    test_res_df = pd.DataFrame(np.array([test_ypred, test_y]).T, columns = ['Fcw_ypred', 'Fcw_yobse'])\n",
    "    test_res_df['ts'] = p1_21_df['ts']\n",
    "    test_ts_res_path_i = f'{datadriven_res_dir}/{estimator_key}_test_ts_res.csv'\n",
    "    test_res_df.to_csv(test_ts_res_path_i)\n",
    "    train_ts_res_path_i = f'{datadriven_res_dir}/{estimator_key}_train_ts_res.csv'\n",
    "    train_res_df.to_csv(train_ts_res_path_i)\n",
    "    grid_res_path_i = f'{datadriven_res_dir}/{estimator_key}_grid_search_res.pkl'\n",
    "    with open(grid_res_path_i, \"wb\") as file:\n",
    "        pickle.dump(grid_res_i, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75aaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_20_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcb26ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor(max_depth=10, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a207d20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6a389",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b617078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
